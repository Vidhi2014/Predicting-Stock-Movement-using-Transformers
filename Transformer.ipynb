{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey7BOUerhzUM",
        "outputId": "dea875b2-62ad-4f33-db03-c5ad818f23fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-21 18:57:55--  https://drive.google.com/uc?export=download&id=1NbH-3LL4bqKmunw388J037IJvXlr_kDW\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.189.102, 64.233.189.101, 64.233.189.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.189.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vj2c7n442t8n2hfs3uvjgt6p6pss5672/1640113050000/14724390508833084599/*/1NbH-3LL4bqKmunw388J037IJvXlr_kDW?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-12-21 18:57:58--  https://doc-0g-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vj2c7n442t8n2hfs3uvjgt6p6pss5672/1640113050000/14724390508833084599/*/1NbH-3LL4bqKmunw388J037IJvXlr_kDW?e=download\n",
            "Resolving doc-0g-18-docs.googleusercontent.com (doc-0g-18-docs.googleusercontent.com)... 142.250.157.132, 2404:6800:4008:c13::84\n",
            "Connecting to doc-0g-18-docs.googleusercontent.com (doc-0g-18-docs.googleusercontent.com)|142.250.157.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20452853 (20M) [text/csv]\n",
            "Saving to: ‘train_data_lag_5.csv’\n",
            "\n",
            "train_data_lag_5.cs 100%[===================>]  19.50M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-12-21 18:57:59 (234 MB/s) - ‘train_data_lag_5.csv’ saved [20452853/20452853]\n",
            "\n",
            "--2021-12-21 18:57:59--  https://drive.google.com/uc?export=download&id=1hz_zmgWPN0mSczWV2m4aq9p4NRNdp4rD\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.189.100, 64.233.189.138, 64.233.189.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.189.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0vj2f2oes2qaafb6obhgbl34eb4mn89u/1640113050000/14724390508833084599/*/1hz_zmgWPN0mSczWV2m4aq9p4NRNdp4rD?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-12-21 18:58:00--  https://doc-0c-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0vj2f2oes2qaafb6obhgbl34eb4mn89u/1640113050000/14724390508833084599/*/1hz_zmgWPN0mSczWV2m4aq9p4NRNdp4rD?e=download\n",
            "Resolving doc-0c-18-docs.googleusercontent.com (doc-0c-18-docs.googleusercontent.com)... 142.250.157.132, 2404:6800:4008:c13::84\n",
            "Connecting to doc-0c-18-docs.googleusercontent.com (doc-0c-18-docs.googleusercontent.com)|142.250.157.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2759042 (2.6M) [text/csv]\n",
            "Saving to: ‘val_data_lag_5.csv’\n",
            "\n",
            "val_data_lag_5.csv  100%[===================>]   2.63M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-12-21 18:58:00 (91.1 MB/s) - ‘val_data_lag_5.csv’ saved [2759042/2759042]\n",
            "\n",
            "--2021-12-21 18:58:01--  https://drive.google.com/uc?export=download&id=1v63Zbg5PizgCqhG9FopdO-i1iWuXFo8D\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.189.113, 64.233.189.100, 64.233.189.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.189.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/cehdpkjqu3o33dp7nlande41uprjs410/1640113050000/14724390508833084599/*/1v63Zbg5PizgCqhG9FopdO-i1iWuXFo8D?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-12-21 18:58:02--  https://doc-08-18-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/cehdpkjqu3o33dp7nlande41uprjs410/1640113050000/14724390508833084599/*/1v63Zbg5PizgCqhG9FopdO-i1iWuXFo8D?e=download\n",
            "Resolving doc-08-18-docs.googleusercontent.com (doc-08-18-docs.googleusercontent.com)... 142.250.157.132, 2404:6800:4008:c13::84\n",
            "Connecting to doc-08-18-docs.googleusercontent.com (doc-08-18-docs.googleusercontent.com)|142.250.157.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3997726 (3.8M) [text/csv]\n",
            "Saving to: ‘test_data_lag_5.csv’\n",
            "\n",
            "test_data_lag_5.csv 100%[===================>]   3.81M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-12-21 18:58:02 (60.2 MB/s) - ‘test_data_lag_5.csv’ saved [3997726/3997726]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1NbH-3LL4bqKmunw388J037IJvXlr_kDW' -O 'train_data_lag_5.csv'\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1hz_zmgWPN0mSczWV2m4aq9p4NRNdp4rD' -O 'val_data_lag_5.csv'\n",
        "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1v63Zbg5PizgCqhG9FopdO-i1iWuXFo8D' -O 'test_data_lag_5.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SRAN3VwmfZdM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "sns.set(color_codes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dc0mgagvfbK1"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train_data_lag_5.csv', lineterminator=\"\\n\")\n",
        "df_train.columns=['stock_date', 'movement percent', 'stock_symbol', 'market_binary', 'tweet']\n",
        "df_train = df_train.drop(columns=['stock_date', 'movement percent', 'stock_symbol'], axis=1)\n",
        "df_val = pd.read_csv('val_data_lag_5.csv', lineterminator=\"\\n\")\n",
        "df_val.columns=['stock_date', 'movement percent', 'stock_symbol', 'market_binary', 'tweet']\n",
        "df_val = df_val.drop(columns=['stock_date', 'movement percent', 'stock_symbol'], axis=1)\n",
        "df_test = pd.read_csv('test_data_lag_5.csv', lineterminator=\"\\n\")\n",
        "df_test.columns=['stock_date', 'movement percent', 'stock_symbol', 'market_binary', 'tweet']\n",
        "df_test = df_test.drop(columns=['stock_date', 'movement percent', 'stock_symbol'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1cOvWzhCfdvh"
      },
      "outputs": [],
      "source": [
        "df_train['market_binary'] = df_train['market_binary'].apply(int)\n",
        "df_test['market_binary'] = df_test['market_binary'].apply(int)\n",
        "df_val['market_binary'] = df_val['market_binary'].apply(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "57VAFf5qiZkk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "hashtags = re.compile(r\"^#\\S+|\\s#\\S+\")\n",
        "mentions = re.compile(r\"^@\\S+|\\s@\\S+\")\n",
        "urls = re.compile(r\"https?://\\S+\")\n",
        "\n",
        "def process_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = hashtags.sub(' hashtag', text)\n",
        "    text = mentions.sub(' entity', text)\n",
        "    return text.strip().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IqlIzMIuiZ63"
      },
      "outputs": [],
      "source": [
        "df_train['tweet'] = df_train.tweet.apply(process_text)\n",
        "df_test['tweet'] = df_test.tweet.apply(process_text)\n",
        "df_val['tweet'] = df_val.tweet.apply(process_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRlv4Ca3mksI",
        "outputId": "7df00524-5945-45bb-b7d7-fb4348811e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install inflect\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PTH1kHFumksJ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import json \n",
        "import inflect\n",
        "import operator\n",
        "def mapNounFrequency(sentenceList):\n",
        "    '''\n",
        "    This function tags entities for a given list of sentences and returns a \n",
        "    frequency map preserving the likeliness of singular and plural occurences.\n",
        "    '''\n",
        "    fMap = {}\n",
        "\n",
        "    if sentenceList == []:\n",
        "        return fMap\n",
        "    \n",
        "    p = inflect.engine()\n",
        "\n",
        "    for sentence in sentenceList:\n",
        "        is_noun = lambda pos: pos[:2] == 'NN'\n",
        "        tokenized = nltk.word_tokenize(sentence)\n",
        "        entities = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)]\n",
        "        for entity in entities.copy():\n",
        "            entities.remove(entity)\n",
        "            if not p.singular_noun(entity):\n",
        "                entities.append(p.plural(entity).lower())\n",
        "            else:\n",
        "                entities.append(entity.lower())\n",
        "        entities = set(entities)\n",
        "\n",
        "        for entity in entities:\n",
        "            if entity in fMap:\n",
        "                fMap[entity] += 1\n",
        "            else:\n",
        "                fMap[entity] = 1\n",
        "    \n",
        "    frequency = list(fMap.values())\n",
        "    normalizer = max(frequency)\n",
        "    for entity,raw in zip(fMap, frequency):\n",
        "        fMap[entity] = int((raw/normalizer)*100)\n",
        "    \n",
        "    return dict( sorted(fMap.items(), key=operator.itemgetter(1),reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "z47gtdLcmksK"
      },
      "outputs": [],
      "source": [
        "def process_vectorized_tweets(tweets, d):\n",
        "  '''\n",
        "  This function is used here to vectorize the tweets using a customized count vectorizer.\n",
        "  '''\n",
        "  vectorized_tweets = []\n",
        "  for tweet in tweets:\n",
        "      vectorized_tweet = []\n",
        "      for word in tweet.split(\" \"):\n",
        "        dict_keys = list(d.keys())\n",
        "        if(word in dict_keys):\n",
        "          vectorized_tweet.append(dict_keys.index(word) + 1)\n",
        "        else:\n",
        "          vectorized_tweet.append(0)\n",
        "      vectorized_tweets.append(vectorized_tweet)\n",
        "  return vectorized_tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcfpYcyGOUw-",
        "outputId": "cb4d88bf-2b6a-4fee-c9be-3358ebb88dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2737\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "vectorized_test_dict = mapNounFrequency(df_test['tweet'].values)\n",
        "vectorized_test_data = process_vectorized_tweets(df_test['tweet'].values, vectorized_test_dict)\n",
        "print(len(vectorized_test_data))\n",
        "with open('test_data.pickle', 'wb') as f:\n",
        "    pickle.dump(vectorized_test_data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw7BREGobavH",
        "outputId": "bff1676b-a840-474f-ba15-2f86038b9abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1981\n"
          ]
        }
      ],
      "source": [
        "vectorized_val_dict = mapNounFrequency(df_val['tweet'].values)\n",
        "vectorized_val_data = process_vectorized_tweets(df_val['tweet'].values, vectorized_val_dict)\n",
        "print(len(vectorized_val_data))\n",
        "with open('val_data.pickle', 'wb') as f:\n",
        "    pickle.dump(vectorized_val_data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UIe8-ndTgArU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27687158-d1bd-499c-897c-cb1092ae8c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14981\n"
          ]
        }
      ],
      "source": [
        "vectorized_train_dict = mapNounFrequency(df_val['tweet'].values)\n",
        "vectorized_train_data = process_vectorized_tweets(df_train['tweet'].values, vectorized_train_dict)\n",
        "print(len(vectorized_train_data))\n",
        "with open('train_data.pickle', 'wb') as f:\n",
        "    pickle.dump(vectorized_train_data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B07nrMB0A14q"
      },
      "outputs": [],
      "source": [
        "# Use this if the vectorized data is already stored\n",
        "# import pickle\n",
        "# with open('test_data.pickle', 'rb') as f:\n",
        "#     vectorized_test_data = pickle.load(f)\n",
        "# with open('val_data.pickle', 'rb') as f:\n",
        "#     vectorized_val_data = pickle.load(f)\n",
        "# with open('train_data.pickle', 'rb') as f:\n",
        "#     vectorized_train_data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gZzjegwPDBXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab2ef73-a3df-4ba8-dee9-19430905f23a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "x_val = np.array(vectorized_val_data)\n",
        "x_train = np.array(vectorized_train_data)\n",
        "x_test = np.array(vectorized_test_data)\n",
        "y_train = np.array(df_train['market_binary'])\n",
        "y_val = np.array(df_val['market_binary'])\n",
        "y_test = np.array(df_test['market_binary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OjbY9El1zCVF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LOha2iKDzE5i"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4H4fk9WezGk0"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6r2cAlYJzIUK"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000  # Only consider the top 10k words\n",
        "maxlen = 200  # Considering character limit for tweet\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen, dtype='float64')\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen, dtype='float64')\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen, dtype='float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KAbdEKzdDDN8"
      },
      "outputs": [],
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5BXhu_HxmksS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42fadafb-6a94-4260-bdd3-33949222d0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, 200, 32)          326400    \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " transformer_block (Transfor  (None, 200, 32)          10656     \n",
            " merBlock)                                                       \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 32)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                660       \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 42        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 337,758\n",
            "Trainable params: 337,758\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82gjbPdXDLdm",
        "outputId": "4fb57291-9f81-40c2-9aa6-525b74a9b938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "469/469 [==============================] - 59s 121ms/step - loss: 0.6954 - accuracy: 0.5066 - val_loss: 0.6963 - val_accuracy: 0.4563\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6932 - accuracy: 0.5122 - val_loss: 0.6936 - val_accuracy: 0.4563\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6939 - accuracy: 0.5071 - val_loss: 0.6906 - val_accuracy: 0.5437\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6934 - accuracy: 0.5124 - val_loss: 0.6965 - val_accuracy: 0.4563\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6931 - accuracy: 0.5116 - val_loss: 0.6945 - val_accuracy: 0.4563\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6930 - accuracy: 0.5136 - val_loss: 0.6962 - val_accuracy: 0.4563\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6931 - accuracy: 0.5114 - val_loss: 0.6940 - val_accuracy: 0.4563\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6982 - val_accuracy: 0.4563\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6930 - accuracy: 0.5122 - val_loss: 0.6944 - val_accuracy: 0.4563\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6943 - val_accuracy: 0.4563\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6931 - accuracy: 0.5123 - val_loss: 0.6951 - val_accuracy: 0.4563\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6929 - accuracy: 0.5114 - val_loss: 0.6942 - val_accuracy: 0.4563\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6929 - accuracy: 0.5127 - val_loss: 0.6947 - val_accuracy: 0.4563\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6930 - accuracy: 0.5115 - val_loss: 0.6957 - val_accuracy: 0.4563\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5118 - val_loss: 0.6956 - val_accuracy: 0.4563\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5118 - val_loss: 0.6953 - val_accuracy: 0.4563\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5122 - val_loss: 0.6943 - val_accuracy: 0.4563\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 54s 116ms/step - loss: 0.6930 - accuracy: 0.5116 - val_loss: 0.6957 - val_accuracy: 0.4563\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 53s 114ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6970 - val_accuracy: 0.4563\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 54s 114ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6950 - val_accuracy: 0.4563\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 53s 114ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6955 - val_accuracy: 0.4563\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 55s 116ms/step - loss: 0.6929 - accuracy: 0.5123 - val_loss: 0.6950 - val_accuracy: 0.4563\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 55s 117ms/step - loss: 0.6929 - accuracy: 0.5119 - val_loss: 0.6945 - val_accuracy: 0.4563\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 55s 118ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6957 - val_accuracy: 0.4563\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 56s 119ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6956 - val_accuracy: 0.4563\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 56s 119ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6951 - val_accuracy: 0.4563\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6962 - val_accuracy: 0.4563\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6956 - val_accuracy: 0.4563\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5139 - val_loss: 0.6941 - val_accuracy: 0.4563\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6930 - accuracy: 0.5112 - val_loss: 0.6953 - val_accuracy: 0.4563\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6959 - val_accuracy: 0.4563\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6952 - val_accuracy: 0.4563\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6948 - val_accuracy: 0.4563\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 57s 123ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6960 - val_accuracy: 0.4563\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6946 - val_accuracy: 0.4563\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5121 - val_loss: 0.6944 - val_accuracy: 0.4563\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6931 - accuracy: 0.5116 - val_loss: 0.6954 - val_accuracy: 0.4563\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6930 - accuracy: 0.5116 - val_loss: 0.6956 - val_accuracy: 0.4563\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5141 - val_loss: 0.6962 - val_accuracy: 0.4563\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6958 - val_accuracy: 0.4563\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6951 - val_accuracy: 0.4563\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6963 - val_accuracy: 0.4563\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6959 - val_accuracy: 0.4563\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 59s 125ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6956 - val_accuracy: 0.4563\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 61s 131ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6949 - val_accuracy: 0.4563\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6960 - val_accuracy: 0.4563\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6956 - val_accuracy: 0.4563\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6956 - val_accuracy: 0.4563\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6960 - val_accuracy: 0.4563\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6965 - val_accuracy: 0.4563\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6960 - val_accuracy: 0.4563\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 73s 156ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6957 - val_accuracy: 0.4563\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 59s 125ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6941 - val_accuracy: 0.4563\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 58s 123ms/step - loss: 0.6930 - accuracy: 0.5112 - val_loss: 0.6957 - val_accuracy: 0.4563\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 58s 124ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6944 - val_accuracy: 0.4563\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6950 - val_accuracy: 0.4563\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6953 - val_accuracy: 0.4563\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6957 - val_accuracy: 0.4563\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6951 - val_accuracy: 0.4563\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6965 - val_accuracy: 0.4563\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6954 - val_accuracy: 0.4563\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6955 - val_accuracy: 0.4563\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6949 - val_accuracy: 0.4563\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 56s 120ms/step - loss: 0.6929 - accuracy: 0.5118 - val_loss: 0.6966 - val_accuracy: 0.4563\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6959 - val_accuracy: 0.4563\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6958 - val_accuracy: 0.4563\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6958 - val_accuracy: 0.4563\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6957 - val_accuracy: 0.4563\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6951 - val_accuracy: 0.4563\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5096 - val_loss: 0.6921 - val_accuracy: 0.5437\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6932 - accuracy: 0.5113 - val_loss: 0.6945 - val_accuracy: 0.4563\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6930 - accuracy: 0.5119 - val_loss: 0.6959 - val_accuracy: 0.4563\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6930 - accuracy: 0.5117 - val_loss: 0.6958 - val_accuracy: 0.4563\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6930 - accuracy: 0.5122 - val_loss: 0.6946 - val_accuracy: 0.4563\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 58s 123ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6953 - val_accuracy: 0.4563\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6948 - val_accuracy: 0.4563\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5105 - val_loss: 0.6955 - val_accuracy: 0.4563\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5119 - val_loss: 0.6953 - val_accuracy: 0.4563\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6962 - val_accuracy: 0.4563\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6949 - val_accuracy: 0.4563\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6962 - val_accuracy: 0.4563\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6948 - val_accuracy: 0.4563\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6961 - val_accuracy: 0.4563\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6952 - val_accuracy: 0.4563\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6947 - val_accuracy: 0.4563\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6945 - val_accuracy: 0.4563\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6951 - val_accuracy: 0.4563\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 58s 123ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6960 - val_accuracy: 0.4563\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 58s 123ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6963 - val_accuracy: 0.4563\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 57s 123ms/step - loss: 0.6931 - accuracy: 0.5118 - val_loss: 0.6958 - val_accuracy: 0.4563\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6945 - val_accuracy: 0.4563\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6930 - accuracy: 0.5119 - val_loss: 0.6951 - val_accuracy: 0.4563\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6952 - val_accuracy: 0.4563\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5121 - val_loss: 0.6956 - val_accuracy: 0.4563\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6950 - val_accuracy: 0.4563\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6930 - accuracy: 0.5121 - val_loss: 0.6952 - val_accuracy: 0.4563\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5118 - val_loss: 0.6957 - val_accuracy: 0.4563\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6947 - val_accuracy: 0.4563\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.6930 - accuracy: 0.5118 - val_loss: 0.6953 - val_accuracy: 0.4563\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 57s 122ms/step - loss: 0.6929 - accuracy: 0.5120 - val_loss: 0.6948 - val_accuracy: 0.4563\n"
          ]
        }
      ],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=100, validation_data=(x_val, y_val)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewWynrj2mksS",
        "outputId": "4178366e-dba9-4283-8a9f-37008e41b9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/86 [==============================] - 4s 47ms/step - loss: 0.6924 - accuracy: 0.5265\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6923729181289673, 0.5264888405799866]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model.evaluate(x_test, y_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "M028Ko-LmksT",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)\n",
        "processed_predictions = []\n",
        "for item in predictions:\n",
        "    if item[0] > item[1]:\n",
        "        processed_predictions.append(0)\n",
        "    else:\n",
        "        processed_predictions.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBlU-rWNmksT",
        "outputId": "df8f93a0-534d-4821-b685-0e5b4c010ffa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_corrcoef(y_test, processed_predictions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Transformer 100 epochs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}